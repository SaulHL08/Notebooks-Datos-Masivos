{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaulHL08/Notebooks-Datos-Masivos/blob/main/FaceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7BP_Oe1iHjo",
        "outputId": "96749232-1e45-48e7-b07c-01dc5582f31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aUgKXv4gba9",
        "outputId": "e7cad984-6dd2-4046-c9fa-130e93b6ae86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han guardado 1241 frames en video1_frames/\n",
            "Se han guardado 1264 frames en video2_frames/\n",
            "Se han guardado 1240 frames en video3_frames/\n",
            "Se han guardado 1413 frames en video4_frames/\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Rutas de los videos en Google Drive\n",
        "video_paths = [\n",
        "    '/content/drive/MyDrive/FaceRecogTrain/Joshua.mp4',\n",
        "    '/content/drive/MyDrive/FaceRecogTrain/Luis.mp4',\n",
        "    '/content/drive/MyDrive/FaceRecogTrain/Marco.mp4',\n",
        "    '/content/drive/MyDrive/FaceRecogTrain/Saul.mp4'\n",
        "]\n",
        "\n",
        "# Nombres de las carpetas de salida\n",
        "output_directories = [\n",
        "    'video1_frames/',\n",
        "    'video2_frames/',\n",
        "    'video3_frames/',\n",
        "    'video4_frames/'\n",
        "]\n",
        "\n",
        "# Crea los directorios si no existen\n",
        "for directory in output_directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Procesa cada video\n",
        "for video_path, output_directory in zip(video_paths, output_directories):\n",
        "    # Lee el video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Inicializa un contador de frames\n",
        "    frame_count = 0\n",
        "\n",
        "    while(cap.isOpened()):\n",
        "        # Lee el frame actual\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            # Guarda el frame como una imagen\n",
        "            frame_path = os.path.join(output_directory, f'frame_{frame_count:04d}.jpg')\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            frame_count += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Libera el objeto VideoCapture\n",
        "    cap.release()\n",
        "\n",
        "    print(f'Se han guardado {frame_count} frames en {output_directory}')\n",
        "\n",
        "# Cierra todas las ventanas\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Comprime la carpeta\n",
        "shutil.make_archive(\"/content/Nombre_Carpeta\", 'zip', \"/content/Nombre_Carpeta\")\n"
      ],
      "metadata": {
        "id": "Cz0MPrkvpWQO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f5bca9cd-fdc6-4c72-91b8-0145f97dcca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Nombre_Carpeta.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread('/content/video4_frames/frame_0909.jpg', cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "print('Original Dimensions : ', img.shape)\n",
        "\n",
        "width = int(img.shape[1] * 28 / 100)\n",
        "height = int(img.shape[0] * 26 / 100)\n",
        "dim = (width, height)\n",
        "\n",
        "resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "cv2.imwrite('/content/video4_frames/frame_0909_resized.jpg', resized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs1FE1v9Arxj",
        "outputId": "cefc7a52-7b1f-4d25-d95b-3e894a3e606d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dimensions :  (1080, 1920, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "onlyfiles = [ f for f in listdir(pathFiles) if isfile(join(pathFiles,f)) ]\n",
        "\n",
        "images = numpy.empty(len(onlyfiles), dtype=object)\n",
        "\n",
        "  for n in range(0, len(onlyfiles)):\n",
        "    #     print(join(pathFiles,onlyfiles[n]))\n",
        "    images[n] = cv2.imread(join(pathFiles,onlyfiles[n]) )"
      ],
      "metadata": {
        "id": "Q0XvBnyDGctL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Paso 1: Cargar las imágenes y etiquetas\n",
        "\n",
        "\n",
        "dataset_dir = \"/content/Dataset\"\n",
        "personas = [\"Joshua\", \"Luis\", \"Saul\", \"Marco\"]\n",
        "etiquetas = []\n",
        "imagenes = []\n",
        "\n",
        "for i, persona in enumerate(personas):\n",
        "    carpeta_persona = os.path.join(dataset_dir, persona)\n",
        "    for archivo in os.listdir(carpeta_persona):\n",
        "        ruta_imagen = os.path.join(carpeta_persona, archivo)\n",
        "        imagen = cv2.imread(ruta_imagen)\n",
        "        imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)  # Convertir a escala de grises\n",
        "        imagen = cv2.resize(imagen, (537, 280))  # Ajusta el tamaño según sea necesario\n",
        "        imagenes.append(imagen)\n",
        "        etiquetas.append(i)\n",
        "\n",
        "imagenes = np.array(imagenes) / 255.0  # Normaliza los valores de píxeles\n",
        "etiquetas = np.array(etiquetas)\n",
        "\n",
        "# Paso 2: Dividir el dataset en entrenamiento y prueba\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(imagenes, etiquetas, test_size=0.2, random_state=42)\n",
        "\n",
        "# Paso 3: Definir y compilar el modelo\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(537, 280, 1)),  # Tamaño ajustado a las imágenes\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(4, activation='softmax')  # 4 para las 4 personas\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Paso 4: Entrenar el modelo\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Paso 5: Evaluar el modelo\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Precisión en el conjunto de prueba: {test_acc}')\n"
      ],
      "metadata": {
        "id": "g7PwDUOK8TWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "save_model(model, \"/content/drive/MyDrive/models/modelo_reconocimiento_facial.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYwwasEa_JWt",
        "outputId": "5ff79952-b48d-4a99-add2-ffe536088376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-8654b7022e35>:3: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, \"/content/drive/MyDrive/models/modelo_reconocimiento_facial.h5\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = load_model(\"/content/drive/MyDrive/models/modelo_reconocimiento_facial.h5\")\n",
        "\n",
        "# Cargar y preprocesar la imagen\n",
        "ruta_imagen_nueva = \"/content/PXL_20231109_213404147.jpg\"  # Reemplaza con la imagen\n",
        "imagen_nueva = cv2.imread(ruta_imagen_nueva)\n",
        "imagen_nueva = cv2.cvtColor(imagen_nueva, cv2.COLOR_BGR2GRAY)  # Convertir a escala de grises\n",
        "imagen_nueva = cv2.resize(imagen_nueva, (100, 100))  # Ajusta el tamaño según sea necesario\n",
        "imagen_nueva = np.array(imagen_nueva) / 255.0  # Normaliza los valores de píxeles\n",
        "imagen_nueva = np.expand_dims(imagen_nueva, axis=0)  # Añade una dimensión adicional para el lote\n",
        "\n",
        "# Realizar la predicción\n",
        "prediccion = model.predict(imagen_nueva)\n",
        "\n",
        "# Obtener la etiqueta predicha\n",
        "etiqueta_predicha = np.argmax(prediccion)\n",
        "\n",
        "# Asignar nombres a las etiquetas\n",
        "nombres_personas = [\"Joshua\", \"Luis\", \"Saul\", \"Marco\"]\n",
        "nombre_predicho = nombres_personas[etiqueta_predicha]\n",
        "\n",
        "print(f\"La imagen pertenece a: {nombre_predicho}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLMNTBe3_q3x",
        "outputId": "54eeabe0-0bb4-4fd4-fba9-4cc8edefe2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 79ms/step\n",
            "La imagen pertenece a: Marco\n"
          ]
        }
      ]
    }
  ]
}